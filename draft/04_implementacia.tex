\section{Návrh metódy}
Navrhovaná metóda zohľadnuje vlastnosti ktoré nie je možné získať iba zo statického obrazu, budeme ich nazívať dynamické príznaky videa.
Avšak metóda stále zohľadnuje v pozorovanom videu aj aspekty statického obrazu, tieto budeme nazívať statické príznaky videa.
Tieto príznaky sú vypočítavané seprátne a nakoniec ich metóda spája do jednej výslednej mapy pozornosti. Výsledkom je postupnosť máp pozornosti pre každý frame videa (podľa vstupnej konfigurácie), ktorý možno spojiť do videa pozornosti pre ľubovolné vstupné video.

\subsection{Dynamické príznaky videa}
Dynamické príznaky metóda najprv extrahuje pomocou štadardnej metódy Horn-Schunck, (referencia na 2 kapitolu alebo na článok?) ktorá vypočíta optický tok na každých 2 rozdielnych framoch videa čím vzniká sémantický príznak pohybu rôznych objektov po scéne spolu s smerovýmy vektormy pohybu daných vektorov.
Získané smerové vektory okamžite spočítavame aby sme získali celkový obraz optického toku pre danú dvojicu obrazov.
Obraz sa následne prahuje statickou konštantou kôli ostráneniu šumu.
Prahovanie prebieha dynamicky vzľadom na počet nájdených 8-spojitých regiónov tj. výstup optikého toku. V našej implementácií je obmedzený počet regiónov na maximálnu hodnotu 200 regiónov.
Prahovanie začne s konštantou ktorú určí pomocou algoritmu Otsu\cite{otsu}, následne určí počet 8-spojitých regiónov akje počet vädší ako maximálna hodnota, zvýši konštantu o 10% z pôvodnej hodnoty.
Tento proces sa opakuje pokial sa v obraze vyskytuje viacej ako maximálny počet regiónov.
Takéto prahovanie je nutné pre optimalizáciu výkonu algoritmu, pretože v prípadoch ked obraz obsahuje veľké množtvo regónov výpočtová rýchlosť algortmu je maximálne neúčinná.
Pixeli s valídnou honotou sa rozdelia na regióny podľa spojitosti a podobnosti štandardným spôsobom.
Pripomenme, že v tomto obraze sa spočítali hodnoty posunu v oboch smeroch aritmeticky do jednej hodnotiacej konštanty (pre každý pixel obrazu), ktorá už nereprezentuje smer posunu daného obrazového pixelu, ale iba hodnotí celkový posun pixelu.
Takto získané regóny budeme vyhodnocovať a spájať podľa pôvodných výsledkov metódy Horn-Schunck.
Vďaka využitiu pôvodných vektorov z výsledku metódy Horn-Schunck, vieme rozlíšiť pohyb horizontálny aj vertikálny separátne.
Pre všetky dvojice regiónov v obraze zistujeme nasledovné charakteristiky:
\begin{enumerate}
  \item\textbf{Rozdiel smerových vektorov v horizontálnom smere}
  \item\textbf{Rozdiel smerových vektorov v vertikálnom smere}
  \item\textbf{Rozdiel vo vzdialenosti}
\end{enumerate}
\subsubsection{Rozdiel smerových vektorov v horizontálnom smere}
Charakteristika sa vypočítava zo smerových horizontálnych vektorov metódy Horn-Schunck.
Pre každý región sa vypočíta maximálna hodnota z indexov daného regiónu.
Následne sa za hodnotu chrakteristiky sa považuje absolútna hodnota rozdielu týchto hodnôt pre každý región.
\begin{equation}
  H_A = max(HS(i_A))
\end{equation}
\begin{equation}
  H_B = max(HS(i_B))
\end{equation}
\begin{equation}
  R_{H} = abs(H_A-H_B)
\end{equation}
Kde A, B reprezentuju všetky dvojice regiónov ktoré sa nachádzajú v obraze.
\begin{math}V_a, V_b\end{math} je maximálna hodnota horizontálnych smerových vektorov z výsledku Horn-Schunck algoritmu pre všetky oblasti patriace danému regiónu.
\begin{math}R_{H}\end{math} je výsledná hodnota charakteristiky.

\subsubsection{Rozdiel smerových vektorov v vertikálnom smere}
Charakteristika sa vypočítava zo smerových vertikálnych vektorov metódy Horn-Schunck.
Prekazdý región sa vypočíta maximálna hodnota z indexov daného regiónu.
Následne sa za hodnotu chrakteristiky sa považuje absolútna hodnota rozdielu týchto hodnôt.

\begin{equation}
  H_A = max(HS(i_A))
\end{equation}
\begin{equation}
  H_B = max(HS(i_B))
\end{equation}
\begin{equation}
  R_{V} = abs(H_A-H_B)
\end{equation}
Kde A, B reprezentuju všetky dvojice regiónov ktoré sa nachádzajú v obraze.
\begin{math}V_a, V_b\end{math} je maximálna hodnota verikálnych smerových vektorov z výsledku Horn-Schunck algoritmu pre všetky oblasti patriace danému regiónu.
\begin{math}R_{V}\end{math} je výsledná hodnota charakteristiky.

\subsubsection{Rozdiel vo vzdialenosti}
Chrakteristika sa vypočítava ako minimálna hodnota vzdialenosti medzi dvojicou regiónov.
Hodnota je počítaná euklidovskou metódou.

\begin{figure}[H]
  \begin{algorithm}[H]
   \ForAll{rohA ako každý extrém regiónu A}{
     \ForAll{rohB ako každý extrém regiónu b}{
      vzdialenost = sqrt( (corner2(1,1)-rohB(1,1))^2 + (rohB(1,2)-rohA(1,2))^2 ) \\
     }
   }
   \caption{Výpočet minimálnej vzdialenosti euklidovskou metódou}
  \end{algorithm}
  \vspace{10mm}
\end{figure}

\subsubsection{Spájanie regiónov}
Po výpočte všetkých 3 charakteristík spojíme všetky dovjice regionov, pre ktoré su všetky chrakteristyky nižšie ako zadefinovaná konštanta.
Regióny spájame pomocou konvexného obalu zjednotenia bodov ležiacich v oboch regiónoch.
\begin{figure}[H]
  \centering
  \includegraphics[width=15cm]{pics/spojenie-regionov.jpg}
  \caption{Vyzualizácia spojenia regiónov pomocou konvexného obalu}
  \vspace{10mm}
\end{figure}

\subsubsection{Starnutie objektov na scéne}
Do vypočitavnia dynamických príznakov započítavame predpoklad, že aj pohybujúce sa objekty postupne strácajú pozornosť používateľov.
A to v prípade kedy sa síce daný objekt na scéne pohybuje, ale na identickom mieste.
do metóty zabudujeme mechanizmus kde pixelom s dlhodobo vysokým hodnotením pozornosti, zmenšíme toto hodnotenie pomocou  vynásobenia koeficientom hodnoty \numrange{0}{1}.

\subsection{Statické príznaky videa}
Pri videách kde sa pohybuje celá scéna (kamera je v pohybe) nedávajú dynamické príznaky dobré výsledky kdeže logicky označia celú scénu alebo jej vädšinovú časť scény za výrazne salientnú.
Preto je vhodné dynamické príznaky vhodne kombinovať s klasickýmy modelmy pozornosti ktoré síce zanedbajú postupnosť obrazov, ale nezlyhajú ako dynamické príznaky.
Pre extrakciu statických obrázkov sme zvolili metódu založnú na spektralnych reziduach\cite{spectral-rezidual}.
Vďaka svojmu príncípu potlačovania štatisticky opakujúcich sa predmetov na scéne, sa dá predpokladaď vhodné doplnenie statických objektov ktoré možu zaujať pozornosť na videu ak zlyhávajú dynamické príznaky.

\subsection{Výsledné spojenie príznakov}
Spájanie dynamických a statických príznakov bude prebiehat pomocou sčítania oboch máp, pričom vždy s použijú v určitom pomere.
Výpočet pomeru bude určovať pomer výskytu salientných pixelov v mape dynamických príznakov.

@TODO add latex som symbol \\
\begin{equation}
  pomer = (sum(P_D} > 0))/Pix_{count}
\end{equation}
Kde \begin{math}P_D\end{math} reprezentuje mapu dynamických príznakov a \begin{math}Pix_{count}\end{math} je počet všetkých pixelov ktoré obraz obsahuje.

Ak je vysoký výskyt salientých pixelov, potrebujeme utlmit zobrazovanie tejto časti príznakov a prioritizovať zobrazovanie statických priznakov preto zmiešavacia funkcia vyzerá nasledovne:

\begin{equation}
  Výsledok = (P_D * (1-pomer)) + (P_S * pomer)
\end{equation}
Kde \begin{math}P_D\end{math} reprezentuje mapu dynamických príznakov a \begin{math}P_S\end{math} mapu statických príznakov.

V prípade, že algoritmus nedokáže detekovať žiadny pohyb na scéne, bol by model pozornosti prázdny.
Preto v prípade keď je vyššie spomýnaný pomer dynamických pixelov extrémne nízky použijeme ako výstup algoritmu iba statické príznaky.
Naopak v prípade, že kamera je v pohybe Horn-Schunck algoritmus označí ako pohybujúci sa vädšinovú oblasť obrazu a v tom prípade je potrebné utlmiť dynamické príznaky obrazu a do popredia vystupujú statické.

\subsection{Zdrojové kodý modelu}
Zdrojový kód obsahuje jednu metódu ktorá príma na vstupe vždy 2 parametre.
Prvý parameter je aktuálny frame videa a druhý parameter je frame videa určený na extrakciu dynamických príznakov videa pomocou differencie vzľadom na prvý obrazový frame.
Tieto 2 obrazové vstupy nemusia byť nutne po sebe idúce, je na používateľovy či použije model serializovane na každý frame videa, alebo zvolí vlastnú implementáciu keyframingu (napríklad kôli časovej náročnosti algoritmu).
Algoritmus je schopný processovať farebné aj čiernobiele obrazové vstupy.
V prílohe je možné nájsť 2 implementácie a to implementáciu ako modelu pre applikáciu na porovnávanie modelov načítavajúca kažké 2 posebeidúce obrazové framy.
Druhá implementácia načítava na vstupe priamo video a na výstup dá video s korešpondujúcim videom význačných oblastí, tátoimplementácia je určená na použitie mimo aplikácie na testovanie.
Obe implementácie sú dostupné v prílohe na CD alebo voľne dostupné na internete.

\clearpage

\subsection{Pipeline metódy}
  Grafický popis metódy obsahujúci príklad zostavenia mapy pozornosti.

  \begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{pics/workflow.jpg}
    \caption{Ucelená vyzualizácia algoritmu}
    \vspace{10mm}
  \end{figure}

\section{Implementácia riešenia}
Implementácia vyššie uvedeného algoritmu je implementovaná ako modul pre aplikáciu na porovnávanie a  automatickú validáciu výsledkov.
Aplikácia na porovnávanie je takisto implementovaná v prostredí matlab.

\subsection{Aplikáciu na porovnávanie a automatickú validáciu}
Sekundárnym prínosom práce je vytvorenie aplikácie pre zjednodušenie budúcej práce pri prototypovaní nových modelov pozornosti.
A následné uľahčenie validačného procesu pre potencionálnych vývojárov.\\
Základná functionalita:
\begin{enumerate}
  \item\textbf{Oddelenie logiky testovnia a logiky samotného modelu}
  \item\textbf{Simultálne sledovanie videa z viacerých modelov}
  \item\textbf{Automatická validácia modelu}
  \item\textbf{Vizualizácia výsledkov validácie}
\end{enumerate}

\subsubsection{Oddelenie logiky testovnia a logiky samotného modelu}
V aplikácií na testovanie je možné pripdávať lubovolné modeli pre ktoré je dostupná implmentácia v jazyku matlab.
Pre iné jazyky je potrebné doprogramovat wrapper ktorý spustí daný jazyk a vypočíta mapu pozornosti.
Ukážkový wrapper je súčastou aplikácie.
Pre pripadnie nového modelu je potrebné pridať wrapper do zložky "models", knižnice vyžadované modelmi je potrebné skopírovat do lubovolnej podzložky tohoto priečinka.
Pri spustení apikácie sa načítaju všetky moduli aj kničnice uložené v podzložkách.

\subsubsection{Simultálne sledovanie videa z viacerých modelov}
Pre rýchle prototypovanie je vhodné pozorovať rovnaké video pri rôznych úpravách.
Táto funkcionalita je dostupná pre každý model s vygenerovanýmy mapamy pozornosti na zvolenom videu.

\subsubsection{Automatická validácia modelu}
Validovanie výsledokov je nutnou súčastou každého modelu pozornosti preto aplikácia ponúka automatizovaný spôsob ako zvalidovať výsledky na vybraných referenčných datasetoch.
Validácia tvorí pre kažké video perzistentný súbor obsahujúci 3 metriky: AUC-Judd, KL-Div, NSS.
Vyššie spomenuté metriky sa rátajú pre každý frame videa.
Validácia prebieha paralelne pre všetky videá zvoleného datasetu.
Vytvorené súbory sú perzistetné z dôvodu dlhého výpočtového času, a ukladajú sa do priečinku results a podložky podľa názvu testovaného datasetu v tvare \begin{math}názovModelu.názovDatasetuČísloVidea.mat\end{math}.
Formát súboru obsahuje 3 premenné s názvamy: AUROC\_score, KLDIV\_score, NSS\_score.
Každá premenná obsahuje pole podla dĺžky videa (počet frameov) a hodnotami danej metriky.
Aplikácia aktuálne podporuje 2 datasety a to: ACCV 2012\cite{accv}, acoutrot( dataset nazov + referencia?), tieto datasety sú volne dostupné a sú súčastou aplikácie (zistit ci su licencie s timto ok).
Dataset ACCV 2012\cite{accv} je poskytovaný autormy aj s testovacím algoritmom na výpočet vyššie uvedených metrík, do aplikácie na testovanie bol iba pozmenený pre načítanie ľubovolného modelu a prisposobený na paralelný vypočet všetkých videí naraz.
Pre dataset acoutrot aplikácie obsahuje upravenú verziu validačného algoritmu z datasetu ACCV 2012.

\subsubsection{Vizualizácia výsledkov validácie}
Pre analýzu výsledkov validácie dokáže aplikácia prehladne vyzualizovať všetky dáta získané testovaním.

\subsection{Implementácia modulu}
Implementácia mového modelu pozornosti je jednoduchá.
Pre integrovanie ľubovolného modelu je možné použiť vzorovú implementáciu ktorá je dostupná v prílohách.

\section{Validácia výsledkov}
Validácia vyššie spomínaného modelu prebiahala pomocou automatického testovania v aplikácií na testovanie.
Validácia potvrdila, že detekcia pohybu a následné optimalizácie majú nenáhodný efekt vzľadom na namerané hodnoty pomocou eyetrackingu viacerých používateľov na roznych videách.

\clearpage

\subsection{Analýza výsledkov}
V tejto sekcií budem prezentovat výsledky všetkých testovaných datasetov.
Výsledky budeme vyzualizovať pomocou chrakteristiky vzniknutej zo strednej hodnoty framov, pre jednotlivé videá datasetov. pre každú metriku samostatným grafom hodnôt.
\\
\vspace{10mm}
Dataset: ACCV 2012\cite{accv}
\begin{figure}[H]
  \includegraphics[width=15cm]{pics/single-accv.jpg}
  \caption{Vyzualizácia všetkých testovaných metrík pre dataset ACCV 2012\cite{accv}}
\end{figure}

\subsubsection{ACCV 2012 - AUCROC}
Podla tejto metriky ...
\subsubsection{ACCV 2012 - KLDIV}
Podla tejto metriky ...
\subsubsection{ACCV 2012 - NSS}
Podla tejto metriky ...

Všetky metriky potvrdzuu tvrdenie, že narvhovaný model ma značnú koreláciu k skutočným dátam nameraných na reálnych užívateľoch.

\\
\\
\\
Dataset: coutrot1\cite{accv}
@TODO images\\
Dataset: coutrot2\cite{accv}
@TODO \\

\subsection{Porovnávanie s konkurenčnýmy modelmy pozornosti}
Porovnanie s konkurenciou nám poskytne dalšie relevantné poznatky.
Budeme sa konkrétne snažiť dokázať ze efektivita našeho nového modelu je vyššia ako povodného horn-struck algoritmu\cite{horn-schunck}, používaného na extrakciu dynamickej zložky príznakov.
Zároveň sa budeme snažiť o dôkaz, že algoritmus je efektívnejší aj ako samotná statická zložka ktorá je vypočítavaná pomocou Spektralnych rezidual\cite{spectral-rezidual}.

Dataset: ACCV 2012\cite{accv}
\begin{figure}[H]
  \includegraphics[width=15cm]{pics/porovnanie-accv.jpg}
  \caption{Vyzualizácia porovnania pre dataset ACCV 2012\cite{accv}}
\end{figure}

\subsubsection{ACCV 2012 - AUCROC}
Podla tejto metriky ...
\subsubsection{ACCV 2012 - KLDIV}
Podla tejto metriky ...
\subsubsection{ACCV 2012 - NSS}
Podla tejto metriky ...

\\
\\
\\
Dataset: coutrot1\cite{accv}
@TODO images\\
Dataset: coutrot2\cite{accv}
@TODO \\

\subsection{Zhrnutie validacie}

\section{Možnosti pre zlepšenie}
\section{Diskusia}
